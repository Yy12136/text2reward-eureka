["import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.0 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 1.0  # Reward for successful grasp\n    else:\n        # Penalize being close to the object without grasping\n        if distance_to_obj < 0.1:\n            reward -= 0.5\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        reward += max(0, 2.0 - 0.5 * distance_to_goal)  # Encourage moving the object closer to the goal\n        \n        # Bonus for reaching the goal\n        if distance_to_goal < 0.05:\n            reward += 2.0\n    \n    # Milestone 4: Keep the object stable during movement\n    if grasp_success:\n        obj_velocity = np.linalg.norm(self.obj.pose.v)\n        reward -= 0.1 * obj_velocity  # Penalize high object velocity\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.01 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.01 * np.linalg.norm(action)\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.5 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Reward for successful grasp\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        reward += max(0, 3.0 - 0.5 * distance_to_goal)  # Encourage moving the object closer to the goal\n    \n    # Milestone 4: Keep the object stable during movement\n    if grasp_success:\n        obj_velocity = np.linalg.norm(self.obj.get_velocity())\n        reward -= 0.1 * obj_velocity  # Penalize high object velocity to encourage stability\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.02 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.02 * np.linalg.norm(action)\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.5 - 0.5 * distance_to_obj)  # Stronger encouragement to move closer to the object\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Higher reward for successful grasp\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        reward += max(0, 3.0 - 0.5 * distance_to_goal)  # Stronger encouragement to move the object closer to the goal\n    \n    # Milestone 4: Object placed at goal\n    if grasp_success and distance_to_goal < 0.05:  # Threshold for successful placement\n        reward += 5.0  # High reward for completing the task\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.02 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.02 * np.linalg.norm(action)\n    \n    # Penalize excessive distance between end-effector and object if not grasping\n    if not grasp_success:\n        reward -= 0.01 * distance_to_obj\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.5 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Reward for successful grasp\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        reward += max(0, 3.0 - 0.5 * distance_to_goal)  # Encourage moving the object closer to the goal\n    \n    # Milestone 4: Object is at the goal\n    if grasp_success and distance_to_goal < 0.01:\n        reward += 5.0  # Large reward for task completion\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.02 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.02 * np.linalg.norm(action)\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reach_reward = max(0, 1.0 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    reward += reach_reward\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Higher reward for successful grasp\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        move_reward = max(0, 3.0 - 0.5 * distance_to_goal)  # Encourage moving the object closer to the goal\n        reward += move_reward\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.1 * np.linalg.norm(joint_velocities)  # Increased penalty for smoother motion\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.05 * np.linalg.norm(action)  # Adjusted penalty for better scaling\n    \n    # Additional penalty for dropping the object (if grasped but not at goal)\n    if grasp_success and distance_to_goal > 0.1:\n        reward -= 0.5  # Penalize if the object is not close to the goal\n    \n    return reward"]