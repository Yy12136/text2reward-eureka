["import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the cube\n    distance_to_cube = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1 - distance_to_cube / 0.1)  # Reward for being close to the cube\n    \n    # Milestone 2: Grasp the cube\n    if grasp_success:\n        reward += 1.0  # Reward for successful grasp\n    else:\n        reward -= 0.1  # Penalty for not grasping\n    \n    # Milestone 3: Lift the cube by 0.2 meters\n    if grasp_success:\n        target_height = obj_pos[2] + 0.2\n        current_height = obj_pos[2]\n        height_diff = target_height - current_height\n        reward += max(0, 1 - abs(height_diff) / 0.2)  # Reward for lifting the cube\n    \n    # Milestone 4: Keep the cube static after lifting\n    if grasp_success and check_actor_static(self.obj):\n        reward += 0.5  # Reward for keeping the cube static\n    \n    # Penalize large actions for smoothness\n    action_penalty = -0.01 * np.linalg.norm(action)\n    reward += action_penalty\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    cubeA_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Move end-effector close to cube A\n    distance_to_cubeA = np.linalg.norm(ee_pos - cubeA_pos)\n    reward += max(0, 1 - distance_to_cubeA / 0.1)  # Reward based on proximity\n    \n    # Milestone 2: Grasp cube A\n    if grasp_success:\n        reward += 1.0  # Reward for successful grasp\n    \n    # Milestone 3: Lift cube A by 0.2 meter\n    if grasp_success:\n        target_height = cubeA_pos[2] + 0.2\n        current_height = cubeA_pos[2]\n        height_diff = abs(target_height - current_height)\n        reward += max(0, 1 - height_diff / 0.2)  # Reward based on height difference\n    \n    # Milestone 4: Ensure cube A is static after lifting\n    if grasp_success and check_actor_static(self.obj):\n        reward += 1.0  # Reward for keeping cube A static\n    \n    # Penalize large actions to encourage smooth movements\n    action_penalty = -0.01 * np.linalg.norm(action)\n    reward += action_penalty\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Move end-effector close to cube A\n    dist_to_cube = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.0 - 5.0 * dist_to_cube)  # Reward decreases with distance\n    \n    # Milestone 2: Grasp cube A\n    if grasp_success:\n        reward += 2.0  # Large reward for successful grasp\n    \n    # Milestone 3: Lift cube A by 0.2 meters\n    if grasp_success:\n        target_height = obj_pos[2] + 0.2\n        height_diff = target_height - obj_pos[2]\n        reward += max(0, 1.0 - 10.0 * abs(height_diff))  # Reward decreases with height difference\n    \n    # Milestone 4: Ensure cube A is static after lifting\n    if grasp_success and check_actor_static(self.obj):\n        reward += 1.0  # Reward for stability\n    \n    # Penalize large actions to encourage smooth movements\n    action_penalty = -0.01 * np.linalg.norm(action)\n    reward += action_penalty\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Step 1: Approach the cube\n    distance_to_cube = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.0 - distance_to_cube) * 0.5  # Encourage getting closer to the cube\n    \n    # Step 2: Grasp the cube\n    if grasp_success:\n        reward += 1.0  # Reward for successful grasp\n    else:\n        reward -= 0.1  # Penalize for not grasping\n    \n    # Step 3: Lift the cube\n    if grasp_success:\n        desired_height = obj_pos[2] + 0.2  # Desired height is 0.2 meters above the current position\n        current_height = obj_pos[2]\n        height_diff = abs(desired_height - current_height)\n        reward += max(0, 1.0 - height_diff) * 0.5  # Encourage lifting to the desired height\n    \n    # Step 4: Ensure the cube is static after lifting\n    if grasp_success and check_actor_static(self.obj):\n        reward += 1.0  # Reward for keeping the cube static after lifting\n    \n    # Penalize for large joint velocities to encourage smooth motion\n    joint_velocities = np.linalg.norm(self.agent.robot.get_qvel()[:-2])\n    reward -= 0.01 * joint_velocities\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    cubeA_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Step 1: Move end-effector close to cube A\n    distance_to_cube = np.linalg.norm(ee_pos - cubeA_pos)\n    reward += 1.0 / (1.0 + distance_to_cube)  # Inverse distance reward\n    \n    # Step 2: Grasp cube A\n    if grasp_success:\n        reward += 1.0  # Bonus for successful grasp\n    else:\n        # Penalize if the end-effector is close but not grasping\n        if distance_to_cube < 0.05:\n            reward -= 0.5\n    \n    # Step 3: Lift cube A by 0.2 meter\n    if grasp_success:\n        target_height = cubeA_pos[2] + 0.2\n        current_height = cubeA_pos[2]\n        height_diff = target_height - current_height\n        reward += 1.0 / (1.0 + abs(height_diff))  # Inverse height difference reward\n        \n        # Bonus for reaching the target height\n        if abs(height_diff) < 0.01:\n            reward += 1.0\n    \n    # Step 4: Ensure cube A is static after lifting\n    if grasp_success and check_actor_static(self.obj):\n        reward += 1.0  # Bonus for keeping the cube static\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_vel = np.linalg.norm(self.agent.robot.get_qvel()[:-2])\n    reward -= 0.1 * joint_vel\n    \n    return reward"]