["import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.5 - 0.75 * distance_to_obj)  # Encourage moving closer to the object\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Reward for successful grasp\n    else:\n        # Penalize being close but not grasping\n        if distance_to_obj < 0.1:\n            reward -= 0.5\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        reward += max(0, 3.0 - 1.0 * distance_to_goal)  # Encourage moving the object closer to the goal\n        \n        # Additional reward for precise placement\n        if distance_to_goal < 0.05:\n            reward += 1.0\n    \n    # Milestone 4: Maintain a stable grasp\n    if grasp_success:\n        # Penalize large joint velocities to encourage smooth motion\n        joint_velocities = self.agent.robot.get_qvel()[:-2]\n        reward -= 0.02 * np.linalg.norm(joint_velocities)\n        \n        # Penalize large actions to encourage efficient movement\n        reward -= 0.02 * np.linalg.norm(action)\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reach_reward = max(0, 1.0 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    reward += reach_reward\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        grasp_reward = 2.0  # Higher reward for successful grasp\n        reward += grasp_reward\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        move_reward = max(0, 3.0 - 0.5 * distance_to_goal)  # Encourage moving the object closer to the goal\n        reward += move_reward\n    \n    # Milestone 4: Keep the object stable near the goal\n    if grasp_success and distance_to_goal < 0.1:\n        stability_reward = 1.0  # Reward for keeping the object stable near the goal\n        reward += stability_reward\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.02 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.02 * np.linalg.norm(action)\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reach_reward = max(0, 1.0 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    reward += reach_reward\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Higher reward for successful grasp\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        move_reward = max(0, 3.0 - 0.5 * distance_to_goal)  # Higher reward for moving closer to the goal\n        reward += move_reward\n    \n    # Milestone 4: Place the object at the goal\n    if grasp_success and distance_to_goal < 0.05:  # Threshold for considering the object placed\n        reward += 5.0  # High reward for placing the object at the goal\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.02 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.02 * np.linalg.norm(action)\n    \n    # Penalize deviations from the optimal path\n    if grasp_success:\n        optimal_path_distance = np.linalg.norm(ee_pos - self.goal_pos)\n        reward -= 0.01 * optimal_path_distance\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reward += max(0, 1.0 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        reward += 2.0  # Higher reward for successful grasp\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        reward += max(0, 3.0 - 0.5 * distance_to_goal)  # Higher reward for moving the object closer to the goal\n    \n    # Milestone 4: Place the object precisely at the goal\n    if grasp_success and distance_to_goal < 0.05:  # Threshold for precise placement\n        reward += 5.0  # High reward for precise placement\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.02 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.02 * np.linalg.norm(action)\n    \n    # Penalize joint position limits to avoid extreme configurations\n    joint_positions = self.agent.robot.get_qpos()[:-2]\n    reward -= 0.01 * np.linalg.norm(joint_positions)\n    \n    return reward", "import numpy as np\n\ndef compute_dense_reward(self, action) -> float:\n    # Initialize reward\n    reward = 0.0\n    \n    # Get positions\n    ee_pos = self.tcp.pose.p\n    obj_pos = self.obj.pose.p\n    \n    # Check grasp\n    grasp_success = self.agent.check_grasp(self.obj)\n    \n    # Milestone 1: Reach the object\n    distance_to_obj = np.linalg.norm(ee_pos - obj_pos)\n    reach_reward = max(0, 1.0 - 0.5 * distance_to_obj)  # Encourage moving closer to the object\n    reward += reach_reward\n    \n    # Milestone 2: Grasp the object\n    if grasp_success:\n        grasp_reward = 2.0  # Higher reward for successful grasp\n        reward += grasp_reward\n    else:\n        # Penalize for not grasping the object\n        reward -= 0.5 * (1.0 - reach_reward)  # Penalize based on how close the end-effector is to the object\n    \n    # Milestone 3: Move the object to the goal\n    if grasp_success:\n        distance_to_goal = np.linalg.norm(obj_pos - self.goal_pos)\n        move_reward = max(0, 3.0 - 0.5 * distance_to_goal)  # Encourage moving the object closer to the goal\n        reward += move_reward\n    else:\n        # Penalize for not moving the object towards the goal\n        reward -= 0.5 * (1.0 - reach_reward)\n    \n    # Milestone 4: Align the end-effector with the object before grasping\n    if not grasp_success:\n        alignment_error = np.linalg.norm(ee_pos - obj_pos)\n        alignment_reward = max(0, 1.0 - 0.5 * alignment_error)\n        reward += alignment_reward\n    \n    # Penalize large joint velocities to encourage smooth motion\n    joint_velocities = self.agent.robot.get_qvel()[:-2]\n    reward -= 0.01 * np.linalg.norm(joint_velocities)\n    \n    # Penalize large actions to encourage efficient movement\n    reward -= 0.01 * np.linalg.norm(action)\n    \n    return reward"]